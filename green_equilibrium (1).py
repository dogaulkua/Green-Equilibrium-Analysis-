# -*- coding: utf-8 -*-
"""Green Equilibrium.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13D05EMUCrJw-iXzHamSvkHSoGdsJMdO7
"""

pip install scikit-fuzzy

!pip install plotly

# Fundamental Libraries
import pandas as pd  # A powerful library for data manipulation and analysis.
import numpy as np  # For numerical operations and array handling.
import os  # For working with files and directories.

# Data Visualization
import matplotlib.pyplot as plt  # For creating static, animated, and interactive visualizations.
import seaborn as sns  # For advanced statistical data visualization.
from matplotlib.colors import ListedColormap  # For creating custom colormaps.

# Mapping and Geospatial Data Handling
import geopandas as gpd  # Extended pandas for working with geospatial data.
import folium  # For creating interactive maps.
from folium.plugins import MarkerCluster, HeatMap, HeatMapWithTime  # Folium plugins for clustering and heatmaps.
import branca.colormap as colormap  # For creating color maps.

# Machine Learning
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score  # For splitting the dataset, model tuning, and cross-validation.
from sklearn.ensemble import RandomForestRegressor  # For regression using the Random Forest algorithm.
from sklearn.metrics import mean_squared_error, r2_score  # For evaluating model performance.
from sklearn.preprocessing import StandardScaler  # For standardizing data.
from sklearn.pipeline import Pipeline  # For streamlining machine learning processes in a pipeline.

from sklearn.metrics import roc_curve, roc_auc_score  # For calculating ROC curve and AUC.
from sklearn.metrics import confusion_matrix  # For creating a confusion matrix.

import skfuzzy as fuzz  # Importing the entire fuzzy logic module.
from skfuzzy import control as ctrl  # Importing the control module for fuzzy logic applications.

import plotly.express as px # For data visualizations

# Utility Libraries
import warnings  # To suppress warnings.
warnings.filterwarnings('ignore')  # Ignore warnings.

from collections import defaultdict  # For creating dictionaries with default values.

# 1. Data Upload Function
def load_data(file_paths):
    data_frames = {}  # Initialize an empty dictionary to store the data frames.
    for name, path in file_paths.items():  # Iterate over the items in the file_paths dictionary.
        try:
            df = pd.read_csv(path)  # Attempt to read the CSV file into a DataFrame.
            data_frames[name] = df  # Store the DataFrame in the dictionary with the corresponding name.
            print(f"{name} dataset loaded. Number of rows: {df.shape[0]}, Number of columns: {df.shape[1]}")  # Print success message with dimensions of the DataFrame.
        except FileNotFoundError:  # Handle the case where the file is not found.
            print(f"Error: {path} not found.")  # Print an error message if the file path is invalid.
    return data_frames  # Return the dictionary containing the loaded DataFrames.

# 3. Initial Review of Datasets
for name, df in data.items():  # Iterate over each dataset in the 'data' dictionary.
    print(f"\n{name} first 5 rows of the dataset:")  # Print the name of the dataset.
    print(df.head())  # Display the first 5 rows of the current dataset (DataFrame).

# File paths for the CSV files
file_paths = {
    'plastic_production': '/content/sample_data/Untitled Folder/plastic-pollution.csv',
    'global_plastic_production': '/content/sample_data/Untitled Folder/global-plastics-production.csv',
    'microplastic_ocean': '/content/sample_data/Untitled Folder/microplastics-in-ocean.csv',
    'marine_microplastic': '/content/sample_data/Untitled Folder/Marine_Microplastics_WGS84_7659999644124488856.csv',
    'filtered_data': '/content/sample_data/Untitled Folder/filtered_data.csv',
    'adventure_micro': '/content/sample_data/Untitled Folder/ADVENTURE_MICRO_FROM_SCIENTIST.csv',
    'geomarine': '/content/sample_data/Untitled Folder/GEOMARINE_MICRO.csv',
}

# Function to load data from CSV files
def load_data(file_paths):
    return {name: pd.read_csv(path) for name, path in file_paths.items()}

# Load the data
data = load_data(file_paths)

# 4. Data Cleaning Functions
def clean_plastic_production(df):
    # Check for the required column for plastic production data.
    required_column = 'Mismanaged plastic waste to ocean per capita (kg per year)'
    if required_column not in df.columns:
        print(f"Error: '{required_column}' column not found.")  # If the column is missing, print an error message.
        return df  # Return the original DataFrame without cleaning.

    # Clean the DataFrame by removing rows with missing or non-positive values in the required column.
    df_cleaned = df.dropna(subset=[required_column])  # Drop rows where the required column has NaN values.
    df_cleaned = df_cleaned[df_cleaned[required_column] > 0]  # Keep only rows where the value is greater than 0.
    return df_cleaned  # Return the cleaned DataFrame.

def clean_filtered_data(df):
    # Check for the required columns for the filtered data.
    required_columns = ['Year', 'Region', 'Plastic Waste Per Capita (kg)']
    missing_columns = [col for col in required_columns if col not in df.columns]  # Identify any missing columns.
    if missing_columns:
        print(f"Error: The following columns were not found: {missing_columns}")  # Print an error if columns are missing.
        return df  # Return the original DataFrame without cleaning.

    # Clean the DataFrame by removing rows with NaN values in the required columns.
    df_cleaned = df.dropna(subset=required_columns)  # Drop rows with NaN in any of the required columns.
    return df_cleaned  # Return the cleaned DataFrame.

def clean_global_plastic_production(df):
    # Check for the required column for global plastic production data.
    required_column = 'Annual plastic production between 1950 and 2019'
    if required_column not in df.columns:
        print(f"Error: '{required_column}' column not found.")  # Print an error if the column is missing.
        return df  # Return the original DataFrame without cleaning.

    # Clean the DataFrame by removing rows with missing or non-positive values in the required column.
    df_cleaned = df.dropna(subset=[required_column])  # Drop rows where the required column has NaN values.
    df_cleaned = df_cleaned[df_cleaned[required_column] > 0]  # Keep only rows with values greater than 0.
    return df_cleaned  # Return the cleaned DataFrame.

def clean_microplastic_ocean(df):
    # Check for the required column for microplastic ocean data.
    required_column = 'Accumulated ocean plastic: Microplastics (<0.5cm)'
    if required_column not in df.columns:
        print(f"Error: '{required_column}' column not found.")  # Print an error if the column is missing.
        return df  # Return the original DataFrame without cleaning.

    # Clean the DataFrame by keeping only rows where the required column has values greater than 0.
    df_cleaned = df[df[required_column] > 0]  # Keep only rows where the value is greater than 0.
    return df_cleaned  # Return the cleaned DataFrame.

def clean_marine_microplastic(df):
    # Check for the required columns for marine microplastic data.
    required_columns = ['Measurement', 'Regions', 'Date']
    missing_columns = [col for col in required_columns if col not in df.columns]  # Identify any missing columns.
    if missing_columns:
        print(f"Error: The following columns were not found: {missing_columns}")  # Print an error if columns are missing.
        return df  # Return the original DataFrame without cleaning.

    try:
        # Clean the 'Measurement' column by converting it to a float after removing non-numeric characters.
        df['Measurement'] = df['Measurement'].astype(str).apply(lambda x: float(re.sub('[^0-9.]', '', x)))  # Remove non-numeric characters.
        print("'Measurement' column cleared successfully.")  # Indicate successful cleaning.
    except Exception as e:
        print("Error occurred while clearing column 'Measurement':", e)  # Handle any errors that occur during cleaning.
    return df  # Return the cleaned DataFrame.

# 5. Implementing Data Cleansing Steps
# Clean the plastic production datasets using the previously defined cleaning functions.
data['plastic_production_cleaned'] = clean_plastic_production(data['plastic_production'])
data['global_plastic_production_cleaned'] = clean_global_plastic_production(data['global_plastic_production'])
data['microplastic_ocean_cleaned'] = clean_microplastic_ocean(data['microplastic_ocean'])
data['marine_microplastic_cleaned'] = clean_marine_microplastic(data['marine_microplastic'])
data['filtered_data'] = clean_filtered_data(data['filtered_data'])

# Define the clean_marine_microplastic function again, possibly for clarity or adjustment.
def clean_marine_microplastic(df):
    # Check for required columns.
    required_columns = ['Measurement', 'Regions', 'Date']
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        print(f"Error: The following columns were not found: {missing_columns}")
        return df  # Return the original DataFrame if any required columns are missing.

    # Clean the 'Measurement' column by replacing empty strings with NaN and dropping NaN values.
    df['Measurement'] = df['Measurement'].replace('', np.nan)  # Replace empty strings with NaN.
    df = df.dropna(subset=['Measurement'])  # Drop rows where 'Measurement' is NaN.

    try:
        # Convert 'Measurement' column to numeric after removing non-numeric characters.
        df['Measurement'] = df['Measurement'].astype(str).apply(lambda x: float(re.sub('[^0-9.]', '', x)))  # Clean the column.
        print("'Measurement' column cleared successfully.")
    except Exception as e:
        print("Error occurred while clearing column 'Measurement':", e)  # Print any errors encountered.
    return df  # Return the cleaned DataFrame.

# Reapply the cleaning functions to ensure data is processed correctly.
data['plastic_production_cleaned'] = clean_plastic_production(data['plastic_production'])
data['global_plastic_production_cleaned'] = clean_global_plastic_production(data['global_plastic_production'])
data['microplastic_ocean_cleaned'] = clean_microplastic_ocean(data['microplastic_ocean'])
data['marine_microplastic_cleaned'] = clean_marine_microplastic(data['marine_microplastic'])
data['filtered_data'] = clean_filtered_data(data['filtered_data'])

# Adding columns to the 'filtered_data' DataFrame if they do not exist.
data['filtered_data']['Year'] = np.nan  # Add 'Year' column with NaN values.
data['filtered_data']['Region'] = 'Unknown'  # Add 'Region' column with the default value 'Unknown'.
data['filtered_data']['Plastic Waste Per Capita (kg)'] = np.nan  # Add 'Plastic Waste Per Capita (kg)' column with NaN values.

print(data['filtered_data'].head())  # Print the first 5 rows of the filtered data.

# Extracting the 'Year' from the 'time' column and adding it to the 'Year' column.
data['filtered_data']['Year'] = pd.to_datetime(data['filtered_data']['time']).dt.year
print(data['filtered_data'].head())  # Print the first 5 rows to check the new 'Year' column.

# A simple classification function for regions based on latitude.
def classify_region(lat):
    if lat > 0:
        return 'Northern Hemisphere'  # Return 'Northern Hemisphere' if latitude is positive.
    else:
        return 'Southern Hemisphere'  # Return 'Southern Hemisphere' if latitude is negative.

# Apply the classification function to the 'lat' column to classify regions.
data['filtered_data']['Region'] = data['filtered_data']['lat'].apply(classify_region)
print(data['filtered_data'].head())  # Print the first 5 rows to check the updated 'Region' classification.

# Optionally, fill NaN values in the 'Plastic Waste Per Capita (kg)' column with a default value of 0.
data['filtered_data']['Plastic Waste Per Capita (kg)'].fillna(0, inplace=True)
print(data['filtered_data'].head())  # Print the first 5 rows to verify the fill operation.

# Reapply the cleaning functions to ensure the data remains consistent.
data['plastic_production_cleaned'] = clean_plastic_production(data['plastic_production'])
data['global_plastic_production_cleaned'] = clean_global_plastic_production(data['global_plastic_production'])
data['microplastic_ocean_cleaned'] = clean_microplastic_ocean(data['microplastic_ocean'])
data['marine_microplastic_cleaned'] = clean_marine_microplastic(data['marine_microplastic'])
data['filtered_data'] = clean_filtered_data(data['filtered_data'])  # Clean filtered data again.

# Check for missing values
for name, df in data.items():  # Iterate over each dataset in the 'data' dictionary
    print(f"\n{name} missing values:\n{df.isnull().sum()}")  # Print the dataset name and the count of missing values in each column

# Selecting numeric columns
numeric_columns = ['OBJECTID', 'Measurement', 'Accession Number', 'Latitude', 'Longitude', 'x', 'y']  # List of column names that are expected to contain numeric data
df_numeric = df[numeric_columns]  # Create a new DataFrame 'df_numeric' that contains only the specified numeric columns from the original DataFrame 'df'

# Select the appropriate cleaned DataFrame
df = data['filtered_data']  # Use the DataFrame you wish to analyze

# Print the columns of the DataFrame to debug
print("Columns in the DataFrame:")  # Print a descriptive message indicating that the following output will list the DataFrame's columns
print(df.columns.tolist())  # Convert the Index object containing column names to a list and print it

# Adjust the numeric_columns list based on your DataFrame
numeric_columns = ['R90P', 'R90D', 'R95P', 'R95D', 'R99P', 'R99D',
                   'drydays', 'wetdays', 'R10mm', 'R20mm', 'RX1Day',
                   'CWD', 'SDII', 'CDD', 'FD', 'SU', 'ID', 'TR',
                   'DTR', 'TX90p', 'TN90p', 'TX10p', 'TN10p', 'HWN',
                   'HWD', 'HWF', 'HWA', 'HWM', 'WSDI', 'LWS', 'CSDI',
                   'LCS', 'RX5Day', 'RX5Daycount', 'Plastic Waste Per Capita (kg)', 'Year', 'lat', 'lon']  # Adjust column names

# Create a numeric DataFrame
df_numeric = df[numeric_columns].select_dtypes(include=[np.number])  # Select only numeric columns

# Check if df_numeric is empty
if df_numeric.empty:
    print("No numeric columns available for correlation analysis.")
else:
    # Calculate the correlation matrix
    correlation_matrix = df_numeric.corr()

# Plot the heatmap
    plt.figure(figsize=(10, 8))
    sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm')
    plt.title('Correlation Heatmap')
    plt.show()

# Assuming you load microplastic_ocean from a CSV or similar file
microplastic_ocean = pd.read_csv('/content/sample_data/Untitled Folder/microplastics-in-ocean.csv')  # Update with your actual file path or data source

# Or ensure it's part of a previous dataset process
# Create new columns based on the 'Accumulated ocean plastic: Microplastics (<0.5cm)' values
microplastic_ocean['microplastic_count'] = microplastic_ocean['Accumulated ocean plastic: Microplastics (<0.5cm)']
microplastic_ocean['is_polluted'] = microplastic_ocean['microplastic_count'].apply(lambda x: 1 if x > 0 else 0)  # Define as polluted if greater than 0

plt.figure(figsize=(10, 6))
sns.scatterplot(data=microplastic_ocean, x='Year', y='microplastic_count', hue='is_polluted', palette='coolwarm')

plt.title('Microplastic Ocean Count over Years (Pollution Highlighted)')
plt.xlabel('Year')
plt.ylabel('Microplastic Count')
plt.legend(title='Is Polluted', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

# Creating sample datasets (5 datasets)
np.random.seed(42)

# Creating plastic production data
df1 = pd.DataFrame({
    'production_ton': np.random.randint(1000, 5000, 100),  # Generate 100 random integers between 1000 and 5000 for plastic production tonnage
    'year': np.random.randint(2000, 2023, 100),  # Generate 100 random integers between 2000 and 2023 for the year
    'is_polluted': np.random.randint(0, 2, 100)  # Generate 100 random integers (0 or 1) indicating pollution status (0 = not polluted, 1 = polluted)
})

# Creating microplastic ocean data
df2 = pd.DataFrame({
    'microplastic_count': np.random.randint(1000, 10000, 100),  # Generate 100 random integers between 1000 and 10000 for microplastic counts
    'year': np.random.randint(2000, 2023, 100),  # Generate 100 random integers between 2000 and 2023 for the year
    'is_polluted': np.random.randint(0, 2, 100)  # Generate 100 random integers (0 or 1) indicating pollution status
})

# Creating marine microplastic data
df3 = pd.DataFrame({
    'marine_microplastic_density': np.random.uniform(0.1, 5.0, 100),  # Generate 100 random float values between 0.1 and 5.0 for marine microplastic density
    'year': np.random.randint(2000, 2023, 100),  # Generate 100 random integers between 2000 and 2023 for the year
    'is_polluted': np.random.randint(0, 2, 100)  # Generate 100 random integers (0 or 1) indicating pollution status
})

# Creating coastal pollution data
df4 = pd.DataFrame({
    'coastal_pollution_index': np.random.uniform(1.0, 10.0, 100),  # Generate 100 random float values between 1.0 and 10.0 for the coastal pollution index
    'year': np.random.randint(2000, 2023, 100),  # Generate 100 random integers between 2000 and 2023 for the year
    'is_polluted': np.random.randint(0, 2, 100)  # Generate 100 random integers (0 or 1) indicating pollution status
})

# Creating waste management data
df5 = pd.DataFrame({
    'waste_management_efficiency': np.random.uniform(50.0, 100.0, 100),  # Generate 100 random float values between 50.0 and 100.0 for waste management efficiency
    'year': np.random.randint(2000, 2023, 100),  # Generate 100 random integers between 2000 and 2023 for the year
    'is_polluted': np.random.randint(0, 2, 100)  # Generate 100 random integers (0 or 1) indicating pollution status
})

# Organizing the datasets into a dictionary for easy access and management
data_sets = {
    'plastic_production': df1,        # Plastic production dataset
    'microplastic_ocean': df2,        # Microplastic count in ocean dataset
    'marine_microplastic': df3,       # Marine microplastic density dataset
    'coastal_pollution': df4,         # Coastal pollution index dataset
    'waste_management': df5            # Waste management efficiency dataset
}

# Accuracy ve AUC değerlerini depolamak için bir sözlük oluşturun
results = {}

# Model eğitimi ve sonuçların depolanması
def model_training(df, dataset_name):
    global accuracy_scores  # Use global variable to store accuracy scores

    X = df.drop('is_polluted', axis=1)  # Features (independent variables)
    y = df['is_polluted']  # Target variable (dependent variable)

    # Handle missing values by imputing with the mean
    imputer = SimpleImputer(strategy='mean')
    X = imputer.fit_transform(X)

    # Split the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Initialize scaler for normalization
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)  # Fit and transform training data
    X_test = scaler.transform(X_test)  # Transform testing data

    # Initialize logistic regression model
    model = LogisticRegression()
    model.fit(X_train, y_train)  # Fit model on training data

    # Predict on test data
    y_pred = model.predict(X_test)

    # Calculate accuracy score
    accuracy = accuracy_score(y_test, y_pred)
    accuracy_scores[dataset_name] = accuracy  # Store accuracy score
    print(f"{dataset_name} Model Accuracy: {accuracy}")  # Print accuracy score
    print(f"{dataset_name} Classification Report:")  # Print classification report header
    print(classification_report(y_test, y_pred))  # Print classification report

    # Calculate predicted probabilities for ROC AUC
    y_pred_proba = model.predict_proba(X_test)[:, 1]  # Positive class probabilities

    # Plot confusion matrix
    plot_confusion_matrix(y_test, y_pred, dataset_name)

    return y_test, y_pred_proba  # Return y_test and predicted probabilities

# Train model for each dataset and store results
for name, df in data_sets.items():
    y_test, y_pred_proba = model_training(df, name)
    auc = roc_auc_score(y_test, y_pred_proba)  # Calculate AUC
    results[name] = {'Accuracy': accuracy_scores[name], 'AUC': auc}

# Print accuracy and AUC scores
print("\nAccuracy and AUC Scores:")
for dataset, metrics in results.items():
    print(f"Dataset: {dataset}, Accuracy: {metrics['Accuracy']}, AUC: {metrics['AUC']}")

def plot_roc_curve(y_test, y_pred_proba, dataset_name):
    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
    auc = roc_auc_score(y_test, y_pred_proba)

    plt.figure(figsize=(6, 4))
    plt.plot(fpr, tpr, label=f"ROC Curve (AUC = {auc:.2f})")
    plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
    plt.title(f"ROC Curve for {dataset_name}")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.legend()
    plt.show()

# Train the model and draw the ROC curve
def model_training_with_roc(df, dataset_name):
    try:
        X = df.drop('is_polluted', axis=1)
        y = df['is_polluted']

        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.transform(X_test)

        model = LogisticRegression()
        model.fit(X_train, y_train)

        # Get predicted probabilities for positive class
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        plot_roc_curve(y_test, y_pred_proba, dataset_name)

        print(f"Model trained and ROC curve plotted successfully for dataset: {dataset_name}")

    except Exception as e:
        print(f"Error processing dataset {dataset_name}: {e}")

# Plot ROC curve for each data set
for name, df in data_sets.items():
    model_training_with_roc(df, name)

# Plot the number of microplastics by year
plt.figure(figsize=(10, 6))  # Create a new figure with specified size
sns.lineplot(data=df2, x='year', y='microplastic_count', marker='o', color='r', label="Microplastic Count")  # Plot the line chart with markers
plt.title("Microplastic Count by Year")  # Set the plot title
plt.xlabel("Year")  # Label the x-axis
plt.ylabel("Microplastic Count")  # Label the y-axis
plt.legend()  # Show the legend
plt.show()  # Display the plot

# Plot the marine microplastic density by year
plt.figure(figsize=(10, 6))  # Create a new figure with specified size
sns.lineplot(data=df3, x='year', y='marine_microplastic_density', marker='o', color='g', label="Marine Microplastic Density")  # Plot the line chart with markers
plt.title("Marine Microplastic Density by Year")  # Set the plot title
plt.xlabel("Year")  # Label the x-axis
plt.ylabel("Microplastic Density (g/m³)")  # Label the y-axis
plt.legend()  # Show the legend
plt.show()  # Display the plot

# Plot the coastal pollution index by year
plt.figure(figsize=(10, 6))  # Create a new figure with specified size
sns.lineplot(data=df4, x='year', y='coastal_pollution_index', marker='o', color='m', label="Coastal Pollution Index")  # Plot the line chart with markers
plt.title("Coastal Pollution Index by Year")  # Set the plot title
plt.xlabel("Year")  # Label the x-axis
plt.ylabel("Pollution Index")  # Label the y-axis
plt.legend()  # Show the legend
plt.show()  # Display the plot

# Plot the waste management efficiency by year
plt.figure(figsize=(10, 6))  # Create a new figure with specified size
sns.lineplot(data=df5, x='year', y='waste_management_efficiency', marker='o', color='b', label="Waste Management Efficiency")  # Plot the line chart with markers
plt.title("Waste Management Efficiency by Year")  # Set the plot title
plt.xlabel("Year")  # Label the x-axis
plt.ylabel("Efficiency (%)")  # Label the y-axis with percentage
plt.legend()  # Show the legend
plt.show()  # Display the plot

# Basic statistics for the DataFrame
print(df1.describe())  # Display summary statistics for all columns in the DataFrame df1

# Missing data analysis
print(df1.isnull().sum())  # Display the count of missing values for each column in the DataFrame df1

# Correlation analysis for the DataFrame df1
plt.figure(figsize=(10, 8))  # Create a new figure with specified size
sns.heatmap(df1.corr(), annot=True, cmap='coolwarm', linewidths=0.5)  # Plot the heatmap of the correlation matrix
plt.title("Correlation Matrix")  # Set the plot title
plt.show()  # Display the plot

# Outlier analysis using a box plot for the DataFrame df1
plt.figure(figsize=(10, 6))  # Create a new figure with specified size
sns.boxplot(x='year', y='production_ton', data=df1)  # Create a box plot to visualize outliers in plastic production over the years
plt.title("Outliers in Plastic Production")  # Set the plot title
plt.show()  # Display the plot

# Analyzing relationships between variables using a pair plot for the DataFrame df1
sns.pairplot(df1)  # Create a pair plot to visualize pairwise relationships in the DataFrame
plt.show()  # Display the plot

# Histogram for the distribution of plastic production in the DataFrame df1
df1['production_ton'].hist(bins=30)  # Create a histogram of the 'production_ton' column with 30 bins
plt.title("Distribution of Plastic Production")  # Set the title of the histogram
plt.xlabel("Tonnage")  # Label for the x-axis
plt.ylabel("Frequency")  # Label for the y-axis
plt.show()  # Display the histogram

# Calculate moving average for plastic production in the DataFrame df1
df1['moving_average'] = df1['production_ton'].rolling(window=3).mean()  # Calculate the 3-year moving average

# Create a line plot to visualize the moving average and actual production
plt.figure(figsize=(10, 6))  # Create a new figure with specified size
sns.lineplot(data=df1, x='year', y='moving_average', marker='o', label="Moving Average")  # Plot the moving average
sns.lineplot(data=df1, x='year', y='production_ton', marker='o', color='orange', label="Plastic Production")  # Plot the actual production data

# Set plot title and labels
plt.title("Plastic Production and Moving Average Over the Years")  # Set the title
plt.xlabel("Year")  # Label for the x-axis
plt.ylabel("Production (Tons)")  # Label for the y-axis
plt.legend()  # Add a legend to the plot
plt.show()  # Display the plot

# Outlier analysis for microplastic count in the DataFrame df2
plt.figure(figsize=(10, 6))  # Create a new figure with specified size
sns.boxplot(data=df2, x='year', y='microplastic_count')  # Create a box plot to visualize outliers in microplastic count over the years

# Set plot title and labels
plt.title("Outliers in Microplastic Count Over the Years")  # Set the title of the plot
plt.xlabel("Year")  # Label for the x-axis
plt.ylabel("Microplastic Count")  # Label for the y-axis
plt.show()  # Display the plot

# Combine multiple datasets into one DataFrame
combined_df = pd.concat([df1, df2, df3, df4, df5], axis=1)  # Merge the datasets along the columns

# Create a heatmap to visualize the correlation matrix
plt.figure(figsize=(12, 10))  # Create a new figure with specified size
sns.heatmap(combined_df.corr(), annot=True, cmap='coolwarm', linewidths=0.5)  # Create a heatmap of the correlation matrix with annotations

# Set plot title
plt.title("Combined Dataset Correlation Matrix")  # Set the title of the plot
plt.show()  # Display the heatmap

adv_sci = gpd.read_file('/content/sample_data/Untitled Folder/ADVENTURE_MICRO_FROM_SCIENTIST.csv',)
geomar = gpd.read_file('/content/sample_data/Untitled Folder/GEOMARINE_MICRO.csv')
sea_micro = gpd.read_file('/content/sample_data/Untitled Folder/Marine_Microplastics_WGS84_7659999644124488856.csv')

def get_geom(df, adn):
    '''Get a geometry variable by combining Latitude and Longitude.'''
    # Ensure Latitude and Longitude are in float format
    df[['Latitude', 'Longitude']] = df[['Latitude', 'Longitude']].astype(dtype=float)

    # Check if the specified additional column exists, if yes convert it to float
    if adn in df.columns:
        df[adn] = df[adn].astype(dtype=float)
    else:
        print(f"Warning: '{adn}' column not found in DataFrame.")  # Warning if the column does not exist

    # Create a new 'Geometry' column as a tuple of (Latitude, Longitude)
    df['Geometry'] = pd.Series([(df.loc[i, 'Latitude'], df.loc[i, 'Longitude']) for i in range(len(df['Latitude']))])

def to_datetime(df, date_col='Date', frmt='%Y-%m-%d'):
    '''Convert the specified date column to datetime and extract the year.'''
    # Check if the specified date column exists
    if date_col in df.columns:
        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')  # Convert to datetime
        df['year'] = df[date_col].dt.year  # Extract the year into a new column
    else:
        print(f"Warning: '{date_col}' column not found in DataFrame.")  # Warning if the column does not exist

# Retrieve geometry for each dataset
get_geom(adv_sci, 'Total_Pieces_L')  # Total number of pieces (length)
get_geom(geomar, 'MP_conc__particles_cubic_metre_')  # Microplastic concentration in particles per cubic meter
get_geom(sea_micro, 'Total_Pieces_KM2')  # Update the column name as needed

# If 'Total_Pieces_KM2' is incorrect, replace it with the correct name:
# get_geom(sea_micro, 'correct_column_name_here')

# Set the date columns to datetime format for each dataset
to_datetime(adv_sci, date_col='correct_date_column_name_here')  # Replace with the actual date column name
to_datetime(geomar, date_col='correct_date_column_name_here')   # Replace with the actual date column name
to_datetime(sea_micro, date_col='correct_date_column_name_here') # Replace with the actual date column name

print(adv_sci.columns)
print(geomar.columns)
print(sea_micro.columns)

#set style for visualization
plt.style.use('seaborn-white')

def plot_hist(arr, label, n_bins=30):
    plt.figure(figsize=(10, 6))
    plt.hist(arr, bins=n_bins, alpha=0.7, color='blue', edgecolor='black')
    plt.title(f'Histogram of {label}')
    plt.xlabel(label)
    plt.ylabel('Frequency')
    plt.grid(axis='y', alpha=0.75)
    plt.show()

plot_hist(arr=adv_sci['Total_Pieces_L'],
          label='Pieces/Litre',
          n_bins=30)

#find loc with most dense

max_plas = adv_sci['Total_Pieces_L'].max()
idx= adv_sci[adv_sci['Total_Pieces_L']==max_plas].index
date1=str(adv_sci.loc[idx,'Date'].values).split('T')[0].split('[')[1]
loc1= adv_sci.iloc[idx][['Latitude','Longitude']].values

print('Maximum plastic found per Litre is {} on date {} at {}'.format(max_plas,date1,loc1))

# Starting position
start_loc = (np.mean(adv_sci['Latitude']), np.mean(adv_sci['Longitude']))

# Creating maps (with OpenStreetMap)
m_1 = folium.Map(location=start_loc,
                  tiles='OpenStreetMap',
                  zoom_start=2,
                  min_zoom=1.5)

# Add attribution for map tile
folium.TileLayer(
    tiles='OpenStreetMap',
    attr='&copy; <a href="https://www.openstreetmap.org/copyright">OpenStreetMap</a> contributors',
).add_to(m_1)

# Add heat map
HeatMap(data=adv_sci[['Latitude', 'Longitude', 'Total_Pieces_L']].values,
         radius=10,
         blur=5).add_to(m_1)

# Add the area with the highest density
loc1 = adv_sci[['Latitude', 'Longitude']].values
folium.CircleMarker(location=(loc1[0][0], loc1[0][1]),
                    tooltip="<b>max plastic density</b>",
                    color='black',
                    radius=15).add_to(m_1)

print('Study area Heatmap (Adventure Scientist)')
m_1

#distribution of data
plot_hist(arr=geomar['MP_conc__particles_cubic_metre_'],
         label='Particles/M3',
         n_bins=100)

# geomar['Datto_listd.to_datetime(geomar['Date'],errors='coerce')

mas_plas = geomar['MP_conc__particles_cubic_metre_'].max()
loc2=geomar[geomar['MP_conc__particles_cubic_metre_']==mas_plas][['Latitude','Longitude']].values

print('Maximum micro plastic concentration found per Cubic Meter is {} at {}'.format(max_plas,
                                                                     loc2))

# Plotting area of study
start_loc = (geomar['Latitude'].mean(), geomar['Longitude'].mean())

# Create the map with attribution for OpenStreetMap tiles
m_3 = folium.Map(location=start_loc,
                 tiles='OpenStreetMap',
                 zoom_start=2,
                 min_zoom=1.5,
                 attr='Map data &copy; <a href="https://www.openstreetmap.org/copyright">OpenStreetMap</a> contributors')

# Add heatmap
HeatMap(data=geomar[['Latitude', 'Longitude', 'MP_conc__particles_cubic_metre_']],
        radius=10,
        blur=5,
        opacity=.1).add_to(m_3)

# Add area of highest concentration
folium.CircleMarker(location=(loc2[0][0], loc2[0][1]),
                    tooltip="<b>max plastic density</b>",
                    color='black',
                    radius=15).add_to(m_3)

# Display the map
print('Study area Heatmap (GeoMarine)')
m_3

m_3 = folium.Map(location=start_loc,
                  tiles='OpenStreetMap',
                  zoom_start=2,
                  min_zoom=1.5)

# Starting position
start_loc = (np.mean(adv_sci['Latitude']), np.mean(adv_sci['Longitude']))

# Creating maps
m_4 = folium.Map(location=start_loc,
                  zoom_start=2,
                  min_zoom=1.5)

# Add OpenStreetMap tile and provide attribution
folium.TileLayer(
    tiles='OpenStreetMap',
    attr='&copy; <a href="https://www.openstreetmap.org/copyright">OpenStreetMap</a> contributors',
).add_to(m_4)

# Add heat map
HeatMap(data=adv_sci[['Latitude', 'Longitude', 'Total_Pieces_L']].values,
         radius=10,
         blur=5).add_to(m_4)

# Add the area with the highest density
loc1 = adv_sci[['Latitude', 'Longitude']].values
folium.CircleMarker(location=(loc1[0][0], loc1[0][1]),
                    tooltip="<b>max plastic density</b>",
                    color='black',
                    radius=15).add_to(m_4)

print('Study area Heatmap (Adventure Scientist)')
m_4

# Sample DataFrame for demonstration (ensure to replace this with your actual DataFrame)
data = {
    'Latitude': [34.0522, 36.7783, 40.7128, 51.5074, 48.8566],  # Example latitudes
    'Longitude': [-118.2437, -119.4179, -74.0060, -0.1278, 2.3522],  # Example longitudes
    'Total_Pieces_L': [10, 20, 30, 40, 50],  # Example data for bubble size
    'year': [2000, 2001, 2002, 2003, 2004]  # Example years
}
df = pd.DataFrame(data)

# Create an animated bubble map
fig = px.scatter_mapbox(
    df,
    lat='Latitude',
    lon='Longitude',
    size='Total_Pieces_L',
    zoom=1,  # Adjust zoom level as needed
    size_max=15,
    mapbox_style="carto-positron",
    animation_frame='year',  # Column to animate
    animation_group='Total_Pieces_L',  # Column to group by during animation
    color='Total_Pieces_L',  # Use this column for color
    color_continuous_scale=px.colors.cyclical.IceFire,  # Choose a color scale for visibility
)

# Show the plot
fig.show()

# Input variables
health_impact = ctrl.Antecedent(np.arange(0, 101, 1), 'health_impact')
gender_equality_impact = ctrl.Antecedent(np.arange(0, 101, 1), 'gender_equality_impact')

# Output variable
overall_impact = ctrl.Consequent(np.arange(0, 101, 1), 'overall_impact')

# Membership functions
health_impact['low'] = fuzz.trimf(health_impact.universe, [0, 0, 50])
health_impact['medium'] = fuzz.trimf(health_impact.universe, [0, 50, 100])
health_impact['high'] = fuzz.trimf(health_impact.universe, [50, 100, 100])

gender_equality_impact['low'] = fuzz.trimf(gender_equality_impact.universe, [0, 0, 50])
gender_equality_impact['medium'] = fuzz.trimf(gender_equality_impact.universe, [0, 50, 100])
gender_equality_impact['high'] = fuzz.trimf(gender_equality_impact.universe, [50, 100, 100])

overall_impact['low'] = fuzz.trimf(overall_impact.universe, [0, 0, 50])
overall_impact['medium'] = fuzz.trimf(overall_impact.universe, [0, 50, 100])
overall_impact['high'] = fuzz.trimf(overall_impact.universe, [50, 100, 100])

#Rules
rule1 = ctrl.Rule(health_impact['low'] & gender_equality_impact['low'], overall_impact['low'])
rule2 = ctrl.Rule(health_impact['medium'] & gender_equality_impact['medium'], overall_impact['medium'])
rule3 = ctrl.Rule(health_impact['high'] & gender_equality_impact['high'], overall_impact['high'])

# Control system
overall_impact_ctrl = ctrl.ControlSystem([rule1, rule2, rule3])
overall_impact_simulation = ctrl.ControlSystemSimulation(overall_impact_ctrl)

# Input values
overall_impact_simulation.input['health_impact'] = 80 # Sample health impact
overall_impact_simulation.input['gender_equality_impact'] = 70 # Example gender equality impact

# Run simulation
overall_impact_simulation.compute()

# Result
print(f"General Impact: {overall_impact_simulation.output['overall_impact']}")
overall_impact.view(sim=overall_impact_simulation)
plt.show()

# Sample microplastic densities (sample data)
concentrations = np.array([0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100])

# Calculate health impact and gender equality impact (sample calculations)
health_impacts = np.array([i * 0.5 for i in concentrations]) # Example health impact calculation
gender_impacts = np.array([i * 0.3 for i in concentrations]) # Example gender equality impact calculation

# Creating the chart
plt.figure(figsize=(12, 6))

# Health impact chart
plt.plot(concentrations, health_impacts, label='Health Impact', color='blue', marker='o')
plt.plot(concentrations, gender_impacts, label='Gender Equality Impact', color='orange', marker='o')

# Title and tags
plt.title('Microplastic Density and Effects')
plt.xlabel('Microplastic Density (%)')
plt.ylabel('Effective Score')
plt.legend()
plt.grid()

# Set the limits of the Y axis (optional)
plt.ylim(0, max(health_impacts.max(), gender_impacts.max()) + 10)

# Show chart
plt.show()

# Calculate health impact and gender equality impact
health_impacts = np.array([i * 0.5 for i in concentrations]) # NumPy array
gender_impacts = np.array([i * 0.3 for i in concentrations]) # NumPy array

# Print the results
print("Health Impact Scores:", health_impacts)
print("Gender Equality Impact Scores:", gender_impacts)

# Visualize the overall impact
plt.figure(figsize=(8, 5))
plt.bar(['Overall Impact'], [overall_impact_simulation.output['overall_impact']], color='lightblue')
plt.ylim(0, 100)
plt.title('General Effect Calculated with Fuzzy Logic')
plt.ylabel('Impact Score')
plt.show()

# Health and gender equality impact scores
health_scores = np.array([0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50])
gender_scores = np.array([0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30])

# Weighted average calculation
overall_scores = (health_scores * 0.6) + (gender_scores * 0.4) # Weights can be changed optionally

print("Overall Impact Scores:", overall_scores)

plt.figure(figsize=(12, 6))
plt.plot(overall_scores, label='Overall Impact', color='green', marker='o')
plt.xlabel('Examples')
plt.ylabel('Overall Impact Scores')
plt.title('Combining Health and Gender Equality Impacts')
plt.legend()
plt.grid()
plt.show()